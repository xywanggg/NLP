{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Language Modeling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.models.Sequential\n",
    "# keras.models.Model\n",
    "\n",
    "# layers: LSTM/ GRU/ Dense/ Dropout/ Embedding/ Bidirectional\n",
    "\n",
    "# keras.preprocessing.sequence.pad_sequences(texts, maxlen=3)\n",
    "    #-->transforms text data into fixed-length vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import required modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# instantiate the model class\n",
    "model = Sequential()\n",
    "\n",
    "# add the layers\n",
    "model.add(Embedding(10000, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "# model.add(Dense(64, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activaton='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# evaluate the model\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# make predictions\n",
    "model.predict(new_data)\n",
    "\n",
    "# summary shows the layers and the number of parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### example\n",
    "# Import relevant classes/functions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60)\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=128, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Method '.evaluate()' shows the loss and accuracy\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Loss: {0} \\nAccuracy: {1}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploding and vanishing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.complie(...., clipvalue=3.0)\n",
    "# use LSTM or GRU cells\n",
    "from keras.layers import GRU, LSTM\n",
    "# add the layers to a model\n",
    "model.add(GRU(units=128, return_sequences=True, name='GRU layer'))\n",
    "model.add(LSTM(units=64, return_sequences=False, name='LSTM layer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "advantages:\n",
    "reduce the dimension / dense representation / transfer learning\n",
    "\n",
    "disadvantages:\n",
    "lots of parameters to train: training takes longer\n",
    "'''\n",
    "from keras.layers import Embedding\n",
    "model = Sequential()\n",
    "#use as the first layer\n",
    "model.add(Embedding(input_dim=100000,\n",
    "                   output_dim=300,\n",
    "                   trainable=True,\n",
    "                   embeddings_inititalizer=None,\n",
    "                   input_length=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transfer learning for language models\n",
    "#GloVE/ word2vec/ BERT\n",
    "\n",
    "#In keras:\n",
    "from keras.initializers import Constant\n",
    "model.add(Embedding(input_dim=vocabulary_size,\n",
    "                   out_dim=embedding_dim,\n",
    "                   embeddings_initializer=Constant(pre_trained_vectors)))\n",
    "###example\n",
    "# Load the glove pre-trained vectors\n",
    "glove_matrix = load_glove('glove_200d.zip')\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n",
    "                    embeddings_initializer=Constant(glove_matrix), \n",
    "                    input_length=sentence_len, trainable=False))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nlp.stanford.edu/projects/glove/\n",
    "####get the GloVE vectors\n",
    "def get_glove_vectors(filename='glove.6B.300d.text'):\n",
    "    #get all word vectors from pre_trained model\n",
    "    glove_vector_dict={}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = values[1:]\n",
    "            glove_vector_dict[word] = np.asarray(coefs, dtype='float32')\n",
    "            \n",
    "###filter GloVE vectors to specific task\n",
    "def filter_glove(vocabulary_dict, glove_dict, wordvec_dim=300):\n",
    "    #create a matrix to store the vectors\n",
    "    embedding_matrix = np.zeros((len(vocabulary_dict)+1, wordvec_dim))\n",
    "    for word, i in vocabulary_dict.items():\n",
    "        embedding_vector = glove_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            #words not found in the glove_dict will be all_zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "improving RNN model\n",
    " 1 add the embedding layer\n",
    " 2 increase the number of layers\n",
    " 3 tune the parameters\n",
    " 4 increase vocabulary size\n",
    " 5 accept longer sentences with more memory cells\n",
    " \n",
    "avoiding overfitting\n",
    " test different batch sizes\n",
    " add dropout layers\n",
    " add dropout and recurrent_dropout parameters on RNN layers\n",
    " add convolution layer, it does feature selection on the embedding vector\n",
    "'''\n",
    "# removes 20% of input to add noise\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "# removes 10% of input and memory cells respectively\n",
    "model.add(LSTM(128, dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# add convolution and pooling\n",
    "model.add(Embedding(vocabulary_size, wordvec_dim,...))\n",
    "model.add(Conv1D(num_filters=32, lernel_size=3, padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example - Sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, wordvec_dim, trainable=True, input_length=max_text_len))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Print the obtained loss and accuracy\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(*model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes from binary classification\n",
    "## shape of the output variable y\n",
    "## number of units on the output layer\n",
    "model.add(Dense(num_classes)) #output layer\n",
    "## activation function on the output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "## loss function\n",
    "model.compile(loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfer learning for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "available architectures\n",
    " gensim:\n",
    "    Word2Vec (CBOW/ Skip-gram)\n",
    "    FastText\n",
    " tensorflow_hub:\n",
    "    ElMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "#Train the model\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=embedding_dim,\n",
    "                             window=neighbor_words-num, iter=100)\n",
    "#get top 3 similar words to 'captain'\n",
    "w2v_model.wv.most_similar(['captain'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import fasttext\n",
    "#instantiate the model\n",
    "ft_model=fasttext.FastText(size=embedding_dim, window=neighbor_word_num)\n",
    "#build vacabulary\n",
    "ft_model.build_vocab(sentences=tokenized_corpus)\n",
    "#train the model\n",
    "ft_model.train(sentences=tokenized_corpus,\n",
    "              total_examples=len(tokenized_corpus),\n",
    "               epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the function to load the data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#download train and test sets\n",
    "new_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "#import modules\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#create and fit the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(news_train.data)\n",
    "\n",
    "#create the (X,Y) variables\n",
    "X_train = tokenizer.texts_to_sequences(news_train.data)\n",
    "X_train = pad_sequences(X_train, maxlen=400)\n",
    "Y_train = to_categorical(news_train.target) #one-hot encoded\n",
    "\n",
    "#train the model\n",
    "model.fit(X_train, Y_train, batch_size=64. epochs=100)\n",
    "\n",
    "#evaluate on test data\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities for each class\n",
    "pred_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "# Thresholds at 0.5 and 0.8\n",
    "y_pred_50 = [np.argmax(x) if np.max(x) >= 0.5 else DEFAULT_CLASS for x in pred_probabilities]\n",
    "y_pred_80 = [np.argmax(x) if np.max(x) >= 0.8 else DEFAULT_CLASS for x in pred_probabilities]\n",
    "\n",
    "trade_off = pd.DataFrame({\n",
    "    'Precision_50': precision_score(y_true, y_pred_50, average=None), \n",
    "    'Precision_80': precision_score(y_true, y_pred_80, average=None), \n",
    "    'Recall_50': recall_score(y_true, y_pred_50, average=None), \n",
    "    'Recall_80': recall_score(y_true, y_pred_80, average=None)}, \n",
    "  index=['Class 1', 'Class 2', 'Class 3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text generation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#similar to a classification model\n",
    "1 uses the vocabulary as classes\n",
    "2 the last layer applies a softmax with wocabulary size units\n",
    "3 uses categorical_crossentropy as loss function\n",
    "\n",
    "#difference to classification\n",
    "1 computes loss, but not performance metrics(accuracy)\n",
    "    -humans see results and evaluate performance\n",
    "    -if not good, train more epochs or add complexity to the model\n",
    "    (add more memory cells, add layers, etc.)\n",
    "2 used with generation rules according to task\n",
    "    -genereate next char/ one word/ one sentence/ one paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(chars_window, n_vocab),\n",
    "              dropout=0.15, recurrent_dropout=0.15, return_sequences=True))\n",
    "model.add(LSTM(units, dropout=dropout, recurrent_dropout=0.15,\n",
    "              return_sequences=False))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Preparing the input text\n",
    "# Get maximum length of the sentences\n",
    "pt_length = max([len(sentence.split()) for sentence in pt_sentences])\n",
    "\n",
    "# Transform text to sequence of numerical indexes\n",
    "X = input_tokenizer.texts_to_sequences(pt_sentences)\n",
    "\n",
    "# Pad the sequences\n",
    "X = pad_sequences(X, maxlen=pt_length, padding='post')\n",
    "\n",
    "# Print first sentence\n",
    "print(pt_sentences[0])\n",
    "\n",
    "# Print transformed sentence\n",
    "print(X[0])\n",
    "\n",
    "\n",
    "################ Preparing the output text\n",
    "# Initialize the variable\n",
    "Y = transform_text_to_sequences(en_sentences, output_tokenizer)\n",
    "\n",
    "# Temporary list\n",
    "ylist = list()\n",
    "for sequence in Y:\n",
    "  \t# One-hot encode sentence and append to list\n",
    "    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))\n",
    "\n",
    "# Update the variable\n",
    "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)\n",
    "\n",
    "# Print the raw sentence and its transformed version\n",
    "print(\"Raw sentence: {0}\\nTransformed: {1}\".format(en_sentences[0], Y[0]))\n",
    "\n",
    "\n",
    "################ Translate Portuguese to English\n",
    "# Function to predict many phrases\n",
    "def predict_many(model, sentences, index_to_word, raw_dataset):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Translate the Portuguese sentence\n",
    "        translation = predict_one(model, sentence, index_to_word)\n",
    "        \n",
    "        # Get the raw Portuguese and English sentences\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        \n",
    "        # Print the correct Portuguese and English sentences and the predicted\n",
    "        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\n",
    "predict_many(model, X_test[0:10], en_index_to_word, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
