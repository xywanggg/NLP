{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match('abc','abdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_regex = '\\w+'\n",
    "re.match(word_regex, 'hi word!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\\w+ word; \\d digit; \\s space; .* wildcard; + or * greedy match;\n",
    "\\S not space; [a-z] lowercase group\n",
    "\n",
    "re module\n",
    "split/ findall/ search/ match\n",
    "\n",
    "| or; (a-z) group; [a-z] explicit character ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = ('(\\d+|\\w+)')\n",
    "re.findall(match, ' He has 11 cats!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('Hi there!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize: #tokenize a document into sentences\n",
    "regexp_tokenize: #tokenize a string or document based on a regular expression pattern\n",
    "TweetTokenizer: #speical class just for tweet tokenization\n",
    "    #allowing you to separate hashtags, mentions and lots of exclamation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = word_tokenize('this is a pretty cool tool!!')\n",
    "word_lengths = [len(w) for w in words]\n",
    "plt.hist(word_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box.\n",
    "The box is over the cat.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization to create a bag of words\n",
    "#lowercasing words\n",
    "#lemmatization/stemming shorten words to their root stems\n",
    "#removing stop words, punctuation or unwanted tokens\n",
    "#good to experiment with different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"The cat is in the box. The cat likes the box.\n",
    "The box is over the cat.\"\"\"\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#building document or word vectors\n",
    "#preforming topic identification and document comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "               'I really liked the movie!',\n",
    "                'Awesome action senes, but boring characers.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidModel(corpus)\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '''In New York, I like to ride the Metro to visit MOMA and\n",
    "some restaurants rated well by Ruth Reichl.'''\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "tagged_sent[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  ride/VB\n",
      "  the/DT\n",
      "  (ORGANIZATION Metro/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (ORGANIZATION MOMA/NNP)\n",
      "  and/CC\n",
      "  some/DT\n",
      "  restaurants/NNS\n",
      "  rated/VBN\n",
      "  well/RB\n",
      "  by/IN\n",
      "  (PERSON Ruth/NNP Reichl/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facus on creating NLP pipelines to generate models and corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.entity\n",
    "doc = nlp(\"\"\"xxxxxx\"\"\")\n",
    "doc.ents  #print entity\n",
    "print(doc.ents[0], doc.ents[0].label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors for many different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''这是一句话'''\n",
    "ptext = Text(text)\n",
    "ptext.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a \"fake news\" classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer\n",
    "\n",
    "df = ... #load data into DataFrame\n",
    "y = df['Sci-Fi']\n",
    "x_train, X_test, y_train, y_test = train_test_split(df['plot'], y,\n",
    "                                                   test_size = 0.33,\n",
    "                                                   random_state=53)\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Naive Bayes classifier\n",
    "#basis in probabiliy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "metrics.accuracy_score(y_test, pred)\n",
    "metrics.confusion_matrix(y_test,pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
