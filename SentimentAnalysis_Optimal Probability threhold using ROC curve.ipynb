{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Phrase\"].tolist()\n",
    "Y = df[\"Sentiment\"].apply(lambda i: 0 if i <= 2 else 1)\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def proc_text(messy): #input is a single string\n",
    "    first = BeautifulSoup(messy, \"lxml\").get_text() #gets text without tags or markup, remove html\n",
    "    second = re.sub(\"[^a-zA-Z]\",\" \",first) #obtain only letters\n",
    "    third = second.lower().split() #obtains a list of words in lower case\n",
    "    fourth = set([lemmatizer.lemmatize(str(x)) for x in third]) #lemmatizing\n",
    "    stops = set(stopwords.words(\"english\")) #faster to search through a set than a list\n",
    "    almost = [w for w in fourth if not w in stops] #remove stop words\n",
    "    final = \" \".join(almost)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [proc_text(i) for i in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=100, test_size=0.2, stratify=Y)\n",
    "print(\"Training Set has {} Positive Labels and {} Negative Labels\".format(sum(y_train), len(y_train) - sum(y_train)))\n",
    "print(\"Test Set has {} Positive Labels and {} Negative Labels\".format(sum(y_test), len(y_test) - sum(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features will be built using tfidf.\n",
    "pos_weights = (len(y_train) - sum(y_train)) / (sum(y_train)) \n",
    "pipeline_tf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=100, class_weight={0: 1, 1: pos_weights}))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model (Before Thresholding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_tf.predict(X_test)\n",
    "predicted_proba = pipeline_tf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score Before Thresholding: {}\".format(accuracy_score(y_test, predictions)))\n",
    "print(\"Precision Score Before Thresholding: {}\".format(precision_score(y_test, predictions)))\n",
    "print(\"Recall Score Before Thresholding: {}\".format(recall_score(y_test, predictions)))\n",
    "print(\"F1 Score Before Thresholding: {}\".format(f1_score(y_test, predictions)))\n",
    "print(\"ROC AUC Score: {}\".format(roc_auc_score(y_test, predicted_proba[:, -1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix of Model (Before Thresholding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = pd.Series(y_test, name='Actual')\n",
    "y_predict_tf = pd.Series(predictions, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actual, y_predict_tf, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_rate, true_pos_rate, proba = roc_curve(y_test, predicted_proba[:, -1])\n",
    "plt.figure()\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\") # plot random curve\n",
    "plt.plot(false_pos_rate, true_pos_rate, marker=\".\", label=f\"AUC = {roc_auc_score(y_test, predicted_proba[:, -1])}\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Optimal Probability Thresholds with ROC Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The curve is plots values of true positive rates (y-axis) against those of false positive rates (x-axis) \n",
    "and these values are plotted at various probability thresholds.\n",
    "There can be two ways of obtaining a more optimal probability threshold for the positive class:\n",
    "\n",
    "##Youden's J Statistic\n",
    "Find the maximum difference between true positive rate and false positive rate \n",
    "and the probability threshold tied tagged to this largest difference would be \n",
    "the selected one\n",
    "\n",
    "Its computed as\n",
    "J = TruePositiveRate + TrueNegativeRate − 1\n",
    "  = TruePositiveRate − FalsePositiveRate\n",
    "\n",
    "\n",
    "##Euclidean Distance\n",
    "The most optimal ROC curve would be one that leans towards the top left of the plot, \n",
    "i.e. true positive rate of 1 and false positive rate of 0.\n",
    "Select the probability threshold as the most optimal one if its true positive rate \n",
    "and false positive rate are closest to the ones mentioned in the previous point \n",
    "in terms of Euclidean distance, i.e.\n",
    "\n",
    "d(tpr,fpr)=[(tpr1−tpr2)sq + (fpr1−fpr2)sq]sq_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the Youden's J statistic to obtain the optimal probability threshold \n",
    "#and this method gives equal weights to both false positives and false negatives\n",
    "optimal_proba_cutoff = sorted(list(zip(np.abs(true_pos_rate - false_pos_rate), proba)), key=lambda i: i[0], reverse=True)[0][1]\n",
    "roc_predictions = [1 if i >= optimal_proba_cutoff else 0 for i in predicted_proba[:, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model (After Thresholding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score Before and After Thresholding: {}, {}\".format(accuracy_score(y_test, predictions), accuracy_score(y_test, roc_predictions)))\n",
    "print(\"Precision Score Before and After Thresholding: {}, {}\".format(precision_score(y_test, predictions), precision_score(y_test, roc_predictions)))\n",
    "print(\"Recall Score Before and After Thresholding: {}, {}\".format(recall_score(y_test, predictions), recall_score(y_test, roc_predictions)))\n",
    "print(\"F1 Score Before and After Thresholding: {}, {}\".format(f1_score(y_test, predictions), f1_score(y_test, roc_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix of Model (After Thresholding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = pd.Series(y_test, name='Actual')\n",
    "y_predict_tf = pd.Series(roc_predictions, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actual, y_predict_tf, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print (df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
