{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introdution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the english language class\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "#create the nlp object\n",
    "nlp = English()\n",
    "\n",
    "#create by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "# iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "# index into the Doc to get a string Token\n",
    "token = doc[1]\n",
    "# get the token text via the .text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "# a slice from the Doc is a span object\n",
    "span = doc[1:4]\n",
    "#get the span text \n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:     [0, 1, 2, 3, 4]\n",
      "Text:      ['It', 'costs', '$', '5', '.']\n",
      "is_alpha:  [True, True, False, False, False]\n",
      "is_punct:  [False, False, False, False, True]\n",
      "like_num:  [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "# lexical attributes\n",
    "doc = nlp(\"It costs $5.\")\n",
    "print('Index:    ',[token.i for token in doc])\n",
    "print('Text:     ',[token.text for token in doc])\n",
    "print('is_alpha: ',[token.is_alpha for token in doc])\n",
    "print('is_punct: ',[token.is_punct for token in doc])\n",
    "print('like_num: ',[token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statistical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Enable spaCy to predict linguistic attributes in context\n",
    "    part of speech tags\n",
    "    suntactic dependencies\n",
    "    name entities\n",
    "trained on labeled example texts\n",
    "can be updated with more examples to fine-tune predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# binary weights\n",
    "# vocabulary\n",
    "# meta information(language, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "# predicting part-of-speech tags\n",
    "doc = nlp('She ate the pizza')\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_) #print predicted part-of-speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "# predicting syntactic dependencies\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# predicting named entities\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_) #print entity text and its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE') #explain method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rule-based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the matcher\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n",
      "Root token: X\n",
      "Root head token: release\n",
      "Previous token: New PROPN\n"
     ]
    }
   ],
   "source": [
    "#add the pattern to the matcher\n",
    "pattern = [{'ORTH': 'iPhone'}, {'ORTH':'X'}]\n",
    "matcher.add('IPHONE_PATTERN',[pattern])\n",
    "\n",
    "#process some text\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "#call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "#iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    #get the span's root token and root head token\n",
    "    print('Root token:', matched_span.root.text)\n",
    "    print('Root head token:', matched_span.root.head.text)\n",
    "    #get the previous tokne and its POS tag\n",
    "    print('Previous token:', doc[start-1].text, doc[start-1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching lexical attributes\n",
    "pattern = [{'IS_DIGI': True},{'LOWER':'fifa'},{'LOWER':'world'},\n",
    "          {'LOWER':'cup'},{'IS_PUNCT':True}]\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "#2018 FIFA World Cup:\n",
    "\n",
    "pattern = [{'LEMMA':'love','POS':'VERB'},{'POS':'NOUN'}]\n",
    "doc = nlp(\"I loved dogs but now I love cats more\")\n",
    "#loved dogs\n",
    "#love cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using operators and quantifiers\n",
    "pattern = [{'LEMMA': 'buy'},\n",
    "           {'POS':'DET','OP':'?'}, #optional:match 0 or 1 times\n",
    "          {'POS':'NOUN'}]\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps\")\n",
    "#bought a smartphone\n",
    "#buying apps\n",
    "\n",
    "#!: 0;   ?: 0 or 1;   +:1 or more;   *: 0 or more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large-scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures: vocab, lexemes and StringStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way 1: 3197928453018144401 coffee\n",
      "way 2: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "#shared vocab and string store\n",
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "print ('way 1:', coffee_hash, coffee_string)\n",
    "\n",
    "doc = nlp('I love coffee')\n",
    "print ('way 2:', doc.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "#lexemes: entries in the vocabulary\n",
    "doc = nlp('I love coffee')\n",
    "lexeme = nlp.vocab['coffee']\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "from spacy.tokens import Doc, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "#create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "#create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "#create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label='GREETING')\n",
    "#add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Doc and Span are very powerful and hold references and relationship of words and sentences\n",
    "-convert result to string as late as possible\n",
    "-use token attributes if available - for expamle, token.i for the token index\n",
    "Don't forget to pass in the sahred vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word vectors and semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "spaCy can compare two objects and predict similarity\n",
    "Doc.similarity(), Span.similarity() and Token.similarity()\n",
    "\n",
    "*need a model that has word vectors included\n",
    "en_core_web_md; en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity examples\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#compare two documents\n",
    "doc1 = nlp('I like fast food')\n",
    "doc2 = nlp('I like pizza')\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "#comapre two tokens\n",
    "doc = nlp('I like pizza and pasta')\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))\n",
    "\n",
    "#comapre a document with a token\n",
    "doc = nlp('I like pizza')\n",
    "token = nlp('soap')[0]\n",
    "print(doc.similarity(token))\n",
    "\n",
    "#comapre a span with a document\n",
    "span = nlp('I like pizza and pasta')[2:5]\n",
    "doc = nlp('McDonalds sells burgers')\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "similarity is determined using word vectors: Word2Vec\n",
    "default: cosine similarity\n",
    "short phrases are better than long ocuments with many irrelevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word vectors in spaCy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp('I have a banana')\n",
    "print(doc[3].vector) #access the vector via the token.vector attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combining models and rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule-based matching\n",
    "# efficient phrase matching\n",
    "''''\n",
    "PhraseMatcher like regular expressions or keyword search\n",
    "-but with access to the tokens\n",
    "takes Doc object as patterns\n",
    "more efficient and faster than the Matcher\n",
    "great for matching large word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span:  Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "pattern = nlp('Golden Retriever')\n",
    "matcher.add('DOG', None, pattern)\n",
    "\n",
    "doc = nlp('I have a Golden Retriever')\n",
    "\n",
    "#iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    #get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span: ', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### built in pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Name    |  Description          | Creates\n",
    "tagger  |part of speech tagger  | Token.tag\n",
    "parser  |dependency parser      | Token.dep, Token.head, Doc.sents, Doc.noun_chunks\n",
    "ner     |named entity recognizer| Doc.ents, Token.ent_iob, Token.ent_type\n",
    "textcat |text classifier        | Doc.cats\n",
    "\n",
    "#nlp.pipe_names: list of pipeline component names\n",
    "#nlp.pipeline: list of (name, component) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def custom_component(doc):\n",
    "    print('Doc length: ', len(doc))\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "print('Pipeline:', nlp.pipe_names)\n",
    "doc = nlp('Hello world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extension attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##add custom metadata to documents, tokens and spans\n",
    "##registered on the global Doc, Token or Span using the set_extension method\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)attribute extensions\n",
    "from spacy.tokens import Token\n",
    "Token.set_extension('is_color', default=False)\n",
    "doc = nlp('The sky is blue.')\n",
    "\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)property extensions\n",
    "\n",
    "##define a getter and an optional setter function\n",
    "##getter only called when you retrieve the attribute value\n",
    "from spacy.tokens import Token\n",
    "def get_is_color(token):\n",
    "    colors = ['red','yellow','blue']\n",
    "    return token.text in colors\n",
    "\n",
    "Token.set_extension('is_color', getter=get_is_color)\n",
    "doc=nlp('The sky is blue')\n",
    "print(doc[3]._.is_color,'-',doc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##span extension should almost always use a getter\n",
    "from spacy.tokens import Span\n",
    "def get_has_color(span):\n",
    "    colors = ['red','yellow','blue']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "Span.set_extension('has_color', getter=get_has_color)\n",
    "doc = nlp('The sky is blue')\n",
    "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3)method extensions\n",
    "\n",
    "##assign a function that becomes available as an object method\n",
    "#lets you pass arguments to the extension function\n",
    "from spacy.tokens import Doc\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "\n",
    "Doc.set_extension('has_token', method=has_token)\n",
    "doc = nlp('The sky is blue.')\n",
    "print(doc._.has_token('blue', '-blue'))\n",
    "print(doc._.has_token('cloud', '-cloud'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scalling and preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing large volumes of text-->use nlp.pipe \n",
    "##bad\n",
    "docs = [nlp(text) for text in lots_of_text]\n",
    "##good\n",
    "docs = list(nlp.pipe(lots_of_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "This is another text 30\n"
     ]
    }
   ],
   "source": [
    "# passing in context\n",
    "data = [('This is a text',{'id':1,'page_num':15}),\n",
    "       ('This is another text', {'id':2,'page_num':30})]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nlp.make_doc to turn a text in to a Doc object\n",
    "##bad\n",
    "doc = nlp('Hello world')\n",
    "##good\n",
    "doc = nlp.make_doc('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nlp.disable_pipes to temporarily disable one or more pipes\n",
    "with nlp.disable_pipes('tagger', 'parser'): #disable tagger and parser\n",
    "    doc=nlp(text) #process the text and print the entities\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### traing and updating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "why updating the model\n",
    "better results on your specific domain\n",
    "learn calssification schemes specifically for your problem\n",
    "essential for text classification\n",
    "very useful for named entity recognition\n",
    "less critical for part-of-speech tagging and dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "how training works\n",
    "1 initialize the model weights readomly with nlp.begin_training\n",
    "2 predict a few examples with the current weights by calling nlp.update\n",
    "3 compare prediction with true labels\n",
    "4 calculate how to change weights to improve predictions\n",
    "5 update weights slightly\n",
    "6 go back to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, 'GADGET') for span in spans]\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    \n",
    "print(*TRAINING_DATA, sep='\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainging loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example loop\n",
    "TRAINING_DATA = [\n",
    "    ('how to preoreder the iPhone X',{'entities':[(20,28,'GADGET')]})\n",
    "    #And many more examples...\n",
    "]\n",
    "for i in range(10):\n",
    "    #shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    #create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        #Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        #update the model\n",
    "        nlp.update(texts, annotations)\n",
    "#save the model\n",
    "nlp.to_disk(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up a new pipeline from scratch\n",
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label('GADGET')\n",
    "\n",
    "#strat the training\n",
    "nlp.begin_training()\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    \n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        \n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)\n",
    "        \n",
    "# Process each text in TEST_DATA\n",
    "for doc in nlp.pipe(TEST_DATA):\n",
    "    # Print the document text and entitites\n",
    "    print(doc.text)\n",
    "    print(doc.ents, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "problem 1: models can forget things\n",
    " -'catastrophic forgetting' problem. existing model can overfit on new data\n",
    "solution 1: mix in previously correct predictions\n",
    "\n",
    "problem 2: models cann't learn everything\n",
    " -spaCy's model make predictions based on local context\n",
    " -model can struggle to learn if decision is difficult to make based on context\n",
    " -label scheme need to be consistent and not too specific\n",
    "solution 2: plan your label scheme carefully\n",
    " -pick categories that are reflected in local context\n",
    " -more generic is better than too specific\n",
    " -use rules to go from generic labels to specific categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
